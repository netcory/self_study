{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597e009d",
   "metadata": {},
   "source": [
    "<img src = 'https://images.manning.com/book/3/8e5d003-09e3-430e-a5a3-f42ee1cafb5f/Stevens-DLPy-HI.png' width = '500' height = '770'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264b208",
   "metadata": {},
   "source": [
    "# 4. Real-world data representation using tensors\n",
    "\n",
    "Here's a question that we can already address: how do we take a piece of data, a video, or a line of text,\n",
    "\n",
    "and represent it with a tensor in a way that is appropriate for training a deep learning model?\n",
    "\n",
    "This is what we'll learn in this chapter.\n",
    "\n",
    "In every section, we will stop where a deep learning researcher would start: right before feeding the data to a model.\n",
    "    \n",
    "We encourage you to keep these datasets; they will constitute excellent material for when we start learning\n",
    "\n",
    "how to train neural network models in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c2af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T03:52:58.114803Z",
     "start_time": "2021-07-01T03:52:58.107810Z"
    }
   },
   "source": [
    "## 4.1 Working with images\n",
    "\n",
    "An image is represented as a collection of scalars arranged in a regular grid with a height and a width (in pixels).\n",
    "\n",
    "We might have a single scalar per grid point (the pixel), which would be represented as a grayscale image; or multiple scalars per grid point,\n",
    "\n",
    "which would typically represent different colors, as we saw in the previous chapter, or different features like depth from a depth camera.\n",
    "\n",
    "### 4.1.1 Adding color channels\n",
    "\n",
    "There are several ways to encode colors into numbers.\n",
    "\n",
    "The most common is RGB.\n",
    "\n",
    "### 4.1.2 Loading an image file\n",
    "\n",
    "Let's start by loading a PNG image using the imageio module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810f01d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T05:56:58.714314Z",
     "start_time": "2021-07-01T05:56:58.597205Z"
    }
   },
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a75b76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T05:56:58.791580Z",
     "start_time": "2021-07-01T05:56:58.775577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24eb8b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T05:56:58.994583Z",
     "start_time": "2021-07-01T05:56:58.955568Z"
    }
   },
   "outputs": [],
   "source": [
    "img_arr = imageio.imread('bobby.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af434460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T05:56:59.164734Z",
     "start_time": "2021-07-01T05:56:59.152731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eaf784",
   "metadata": {},
   "source": [
    "(Note: For many purposes, using TorchVision is a great default choice to deal with image and video data.\n",
    "\n",
    "We go with imageio here for somewhat lighter exploration.)\n",
    "\n",
    "At this point, img is a NumPy array-like object with three dimensions:\n",
    "    \n",
    "two spatial dimensions, width and height; and a third dimension corresponding to the red, green, and blue channels.\n",
    "\n",
    "Any library that outputs a NumPy array will suffic to obtain a PyTorch tensor.\n",
    "\n",
    "The only thing to watch out for is the layout of the dimensions.\n",
    "\n",
    "PyTorch modules dealing with image data require tensors to be laid out as\n",
    "\n",
    "C x H x W: channels, height, and width. respectively.\n",
    "    \n",
    "### 4.1.3 Changing the layout\n",
    "\n",
    "We can use the tensor's permute method with the old dimensions for each new dimension\n",
    "\n",
    "to get to an appropriate layout.\n",
    "\n",
    "Given an input tensor H x W x C as obtained previously,\n",
    "\n",
    "we get a proper layout by having channel 2 first and then channels 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da60f420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T05:57:01.474071Z",
     "start_time": "2021-07-01T05:57:01.121011Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "img = torch.from_numpy(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5675bd39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T05:57:01.489855Z",
     "start_time": "2021-07-01T05:57:01.476072Z"
    }
   },
   "outputs": [],
   "source": [
    "out = img.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb3bd6",
   "metadata": {},
   "source": [
    "As a slightly more effiecient alternative to using 'stack' to build up the tensor,\n",
    "\n",
    "we can pre-allocate a tensor of appropriate size and fill it with images loaded from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc0c675b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T06:04:32.371069Z",
     "start_time": "2021-07-01T06:04:32.322515Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f852f32",
   "metadata": {},
   "source": [
    "This indicates that our batch will consist of three RGB images 256 pixels in height\n",
    "\n",
    "and 256 pixels in width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09671a46",
   "metadata": {},
   "source": [
    "### 4.1.4 Normalizing the data\n",
    "\n",
    "Neural networks exhibit the best training performance when the input data\n",
    "\n",
    "ranges roughly from 0 to 1, from -1 to 1 (this is an effect of how their building blokcks are defined).\n",
    "\n",
    "So a typical thing we'll want to do is cast a tensor to floating-point and normalize the values of the pixles.\n",
    "\n",
    "Casting to floating-point is easy, but normalization is trickier.\n",
    "\n",
    "One possibility is to just divide the values of the pixels by 255\n",
    "(the maximum representable number in 8-bit unsigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe3dfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T06:15:46.270618Z",
     "start_time": "2021-07-01T06:15:46.265628Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = batch.float()\n",
    "batch /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68adcd",
   "metadata": {},
   "source": [
    "Another possibility is to compute the mean and standard deviation of the input data\n",
    "\n",
    "and scale it so that the output has zero mean and unit standard deviation across each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a03e6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T06:14:44.682949Z",
     "start_time": "2021-07-01T06:14:44.648513Z"
    }
   },
   "outputs": [],
   "source": [
    "n_channels = batch.shape[1]\n",
    "\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] -mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6989aa",
   "metadata": {},
   "source": [
    "## 4.2 3D images: Volumetric data\n",
    "\n",
    "### 4.2.1 Loading a specialized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba7d7fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T06:25:51.830566Z",
     "start_time": "2021-07-01T06:25:51.581279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 78/99  (78.899/99  (100.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "dir_path = 'data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083'\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "048fd435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T06:48:33.115591Z",
     "start_time": "2021-07-01T06:48:33.087555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99, 512, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_arr).float()\n",
    "vol = torch.unsqueeze(vol, 0)\n",
    "\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eae587",
   "metadata": {},
   "source": [
    "## 4.3 Reprsenting tabular data\n",
    "\n",
    "The simplest form of data we'll encounter on a machine learning job\n",
    "\n",
    "is sitting in a spreadsheet, CSV file, or database.\n",
    "\n",
    "### 4.3.1 Using a real-world dataset\n",
    "\n",
    "Our first job as deep learning practitioners is to encode heterogeneous,\n",
    "\n",
    "real-world data into a tensor of floatig-point numbers, read for consumption by a neural network.\n",
    "\n",
    "Let's start with something fun: wine!\n",
    "\n",
    "The file contains a comma-separated collection of values\n",
    "\n",
    "organized in 12 columns preceded by a header line containing the column names.\n",
    "\n",
    "The first 11 columns contain values of chemical variables,\n",
    "\n",
    "and the last column contains the sensory quality score\n",
    "\n",
    "from 0 (very bad) to 10 (excellent).\n",
    "\n",
    "Pythoon offers several options for quickly loading a CSV file.\n",
    "\n",
    "Three popular options are\n",
    "\n",
    "- csv module\n",
    "\n",
    "- NumPy\n",
    "\n",
    "- Pandas\n",
    "\n",
    "The third option is the most time- and memory-efficient.\n",
    "\n",
    "However, we'll avoid introducing an additional library in our learning trajectory\n",
    "\n",
    "just because we need to load a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "656c9afd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:00:04.887023Z",
     "start_time": "2021-07-01T07:00:04.797501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "wine_path = 'data/p1ch4/tabular-wine/winequality-white.csv'\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=';', skiprows=1)\n",
    "\n",
    "wineq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb350a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:00:29.005520Z",
     "start_time": "2021-07-01T07:00:28.996513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4898, 12),\n",
       " ['fixed acidity',\n",
       "  'volatile acidity',\n",
       "  'citric acid',\n",
       "  'residual sugar',\n",
       "  'chlorides',\n",
       "  'free sulfur dioxide',\n",
       "  'total sulfur dioxide',\n",
       "  'density',\n",
       "  'pH',\n",
       "  'sulphates',\n",
       "  'alcohol',\n",
       "  'quality'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
    "\n",
    "wineq_numpy.shape, col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab408784",
   "metadata": {},
   "source": [
    "and proceed to convert the NumPy array to a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aebb4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:01:32.866057Z",
     "start_time": "2021-07-01T07:01:32.850506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4357a0",
   "metadata": {},
   "source": [
    "### 4.3.3 Representing scores\n",
    "\n",
    "We will typicallly remove the score from the tensor of input data\n",
    "\n",
    "and keep it in a separate tensor, so that we can use the score as the groun truth\n",
    "\n",
    "without it being input to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0823e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:04:02.372694Z",
     "start_time": "2021-07-01T07:04:02.354699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
       " torch.Size([4898, 11]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = wineq[:, :-1] # selects all rows and all columns except the last\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba9da862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:05:39.734440Z",
     "start_time": "2021-07-01T07:05:39.718426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target= wineq[:, -1] # selects all rows and the last column\n",
    "target, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86c7da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:06:08.985584Z",
     "start_time": "2021-07-01T07:06:08.973076Z"
    }
   },
   "source": [
    "If we want to transform the target tensor in a tensor of labels, we have two options.\n",
    "\n",
    "One is simply to treat labels as an integer vector of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6f07842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:06:12.391053Z",
     "start_time": "2021-07-01T07:06:12.373114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = wineq[:, -1].long()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b03a7",
   "metadata": {},
   "source": [
    "### 4.3.4 One-hot encoding\n",
    "\n",
    "The other approach is to build a one-hot encoding of the scores:\n",
    "    \n",
    "that is, encode each of the 10 scores in a vector of 10 elements,\n",
    "\n",
    "with all elements set to 0 but one, at a different index for each score.\n",
    "\n",
    "We can achieve one-hot encoding using the 'scatter_' method,\n",
    "\n",
    "which fills the tensor with values from a source tensor along the indices provided as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ec9867a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:09:23.890349Z",
     "start_time": "2021-07-01T07:09:23.872842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)\n",
    "# unsqueeze 함수는 1인 차원을 생성하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f2283",
   "metadata": {},
   "source": [
    "The arguments for scatter_ are as follows\n",
    "\n",
    "- The dimension along which the following two arguments are specified.\n",
    "\n",
    "- A column tensor indicating the indices of the elements to scatter\n",
    "\n",
    "- A tensor containing the elements to scatter or a single scalar to scatter (1, in this case)\n",
    "\n",
    "In other words, the previous invocation reads,\n",
    "\n",
    "\"For each row, take the index of the target label (which coincides with the score in our case)\n",
    "\n",
    "and use it as the columns index to set the valu 1.0.\"\n",
    "\n",
    "The second argument of scatter_, the index tensor, is required to have\n",
    "\n",
    "the same number of dimensions as the tensor we scatter into.\n",
    "\n",
    "Since target_onehot has two dimensions (4,898 x 10),\n",
    "\n",
    "we need to add an extra dummy dimension to target using unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a02c1ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:17:07.195472Z",
     "start_time": "2021-07-01T07:17:07.185470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        ...,\n",
       "        [6],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_unsqueezed = target.unsqueeze(1)\n",
    "target_unsqueezed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633ef84",
   "metadata": {},
   "source": [
    "### 4.3.5 When to categorize\n",
    "\n",
    "Let's go back to our 'data' tensor, containing 11 variables associated with\n",
    "\n",
    "the chemical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a7d1f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:19:30.703454Z",
     "start_time": "2021-07-01T07:19:30.689952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = torch.mean(data, dim=0)\n",
    "data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9d7ab4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:19:55.640798Z",
     "start_time": "2021-07-01T07:19:55.615498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
       "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_var = torch.var(data, dim=0)\n",
    "data_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44d27269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:37:15.009482Z",
     "start_time": "2021-07-01T07:37:14.992486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7208e-01, -8.1761e-02,  2.1326e-01,  ..., -1.2468e+00,\n",
       "         -3.4915e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7996e-02,  ...,  7.3995e-01,\n",
       "          1.3422e-03, -8.2419e-01],\n",
       "        [ 1.4756e+00,  1.7450e-02,  5.4378e-01,  ...,  4.7505e-01,\n",
       "         -4.3677e-01, -3.3663e-01],\n",
       "        ...,\n",
       "        [-4.2043e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3130e+00,\n",
       "         -2.6153e-01, -9.0545e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0049e+00,\n",
       "         -9.6251e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7505e-01,\n",
       "         -1.4882e+00,  1.0448e+00]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normalized = (data - data_mean) / torch.sqrt(data_var)\n",
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d266823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:38:29.840539Z",
     "start_time": "2021-07-01T07:38:29.823525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), torch.bool, tensor(20))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 4.3.6 Finding thresholds\n",
    "\n",
    "bad_indexes = target <= 3\n",
    "bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd7e1c",
   "metadata": {},
   "source": [
    "Note that only 20 of the bad_indexs entries are set to True!\n",
    "\n",
    "By using a feature in PyTorch called advanced indexing,\n",
    "\n",
    "we can use a tensor with data type torch.bool to index the data tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c77aff84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:40:15.244861Z",
     "start_time": "2021-07-01T07:40:15.228857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 11])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data = data[bad_indexes]\n",
    "bad_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f3364e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:43:14.790518Z",
     "start_time": "2021-07-01T07:43:14.766998Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_data = data[target <= 3]\n",
    "mid_data = data[(target > 3) & (target <7)]\n",
    "good_data = data[target >= 7]\n",
    "\n",
    "bad_mean = torch.mean(bad_data, dim=0)\n",
    "mid_mean = torch.mean(mid_data, dim=0)\n",
    "good_mean = torch.mean(good_data, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67c8c173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:46:48.154120Z",
     "start_time": "2021-07-01T07:46:48.142117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 fixed acidity          7.60   6.89   6.73\n",
      " 1 volatile acidity       0.33   0.28   0.27\n",
      " 2 citric acid            0.34   0.34   0.33\n",
      " 3 residual sugar         6.39   6.71   5.26\n",
      " 4 chlorides              0.05   0.05   0.04\n",
      " 5 free sulfur dioxide   53.33  35.42  34.55\n",
      " 6 total sulfur dioxide 170.60 141.83 125.25\n",
      " 7 density                0.99   0.99   0.99\n",
      " 8 pH                     3.19   3.18   3.22\n",
      " 9 sulphates              0.47   0.49   0.50\n",
      "10 alcohol               10.34  10.26  11.42\n"
     ]
    }
   ],
   "source": [
    "for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n",
    "    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69333260",
   "metadata": {},
   "source": [
    "Let's get the indexes where the total sulfur dioxide colummn is below the midpoint we calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4bd85849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:53:02.458396Z",
     "start_time": "2021-07-01T07:53:02.449394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), torch.bool, tensor(2727))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sulfur_threshold = 141.83\n",
    "total_sulfur_data = data[:, 6]\n",
    "predicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold)\n",
    "\n",
    "# torch.lt(input, other, *, out)\n",
    "# input: tensor to compare, other: the tensor or value to compare\n",
    "# out: outpput tensor\n",
    "# it returns A boolean tensor that is True where input is less than other and False elsewhere\n",
    "\n",
    "predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36884075",
   "metadata": {},
   "source": [
    "This means our threshold implies that just over half of all the wines are\n",
    "\n",
    "going to be high quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29797265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:56:36.184888Z",
     "start_time": "2021-07-01T07:56:36.175351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), torch.bool, tensor(3258))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_indexes = target > 5\n",
    "\n",
    "actual_indexes.shape, actual_indexes.dtype, actual_indexes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9dae82",
   "metadata": {},
   "source": [
    "Since there are about 500 more actually good wines than our threshold predicted,\n",
    "\n",
    "we already have hard evidence that it's not perfect.\n",
    "\n",
    "Now we need to see how well our predictions line up with the actual rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c9185c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T07:59:03.707648Z",
     "start_time": "2021-07-01T07:59:03.696646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 0.74000733406674, 0.6193984039287906)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_matches = torch.sum(actual_indexes & predicted_indexes).item()\n",
    "n_predicted = torch.sum(predicted_indexes).item()\n",
    "n_actual = torch.sum(actual_indexes).item()\n",
    "\n",
    "n_matches, n_matches / n_predicted, n_matches / n_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c21dc",
   "metadata": {},
   "source": [
    "We got around 2,000 wines right!\n",
    "\n",
    "Since we predicted 2,700 wines, this gives us a 74% chance\n",
    "\n",
    "that if we predict a wine to be high quality, it actually is.\n",
    "\n",
    "Unfortunately, there are 3,200 good wines, and we only identified 61% of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07813c",
   "metadata": {},
   "source": [
    "## 4.4 Working with time series\n",
    "\n",
    "We'll switch to another interesting dataset: data from Washington, D.C.,\n",
    "\n",
    "bike-sharing system reporting the hourly count of rental bikes in 2011-2012\n",
    "\n",
    "in the Capital Bikeshare system. along with weather and seasonal information.\n",
    "\n",
    "Our goal will be to take a flat, 2D dataset and transform it into a 3D one.\n",
    "\n",
    "### 4.4.1 Adding a time dimension\n",
    "\n",
    "In the source data, each row is a separate hour of data.\n",
    "\n",
    "We want to change the row-per-hor organization so that\n",
    "\n",
    "we have one axis that increases at a rate of one day per index increment,\n",
    "\n",
    "and another axis that represents the hour of the day.\n",
    "\n",
    "The third axis will be our different columns of data (weather, temperature, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "255e0568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:08:01.303223Z",
     "start_time": "2021-07-01T08:08:00.956394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
       "         1.6000e+01],\n",
       "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
       "         4.0000e+01],\n",
       "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
       "         3.2000e+01],\n",
       "        ...,\n",
       "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
       "         9.0000e+01],\n",
       "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
       "         6.1000e+01],\n",
       "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
       "         4.9000e+01]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes_numpy = np.loadtxt('data/p1ch4/bike-sharing-dataset/hour-fixed.csv',\n",
    "                         dtype=np.float32,\n",
    "                         delimiter=',',\n",
    "                         skiprows=1,\n",
    "                         converters={1: lambda x: float(x[8:10])})\n",
    "\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96b28b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:08:38.508719Z",
     "start_time": "2021-07-01T08:08:38.502707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17520, 17])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ab2c4",
   "metadata": {},
   "source": [
    "For every hour, the dataset reports the following variables\n",
    "\n",
    "- Index of record: instant\n",
    "- Day of month: day\n",
    "- Season: season(1: spring, 2: summer, 3: falll, 4: winter)\n",
    "\n",
    "...\n",
    "\n",
    "- Count of rental bikes: cnt\n",
    "\n",
    "In a time series dataset such as this one, rows represent successive time-points:\n",
    "\n",
    "there is a dimension along which they are ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597542f",
   "metadata": {},
   "source": [
    "### 4.4.2 Shaping the data by time period\n",
    "\n",
    "We might want to break up the two-year dataset into wider observation periods, like days.\n",
    "\n",
    "Thhis way we'll have N (for number of samples) collections of C sequences of length L.\n",
    "\n",
    "In other words, our time series dataset would be a tensor of dimension 3 and shape N x C x L.\n",
    "\n",
    "The C would remain our 17 channels, while L would be 24: 1 per hour of the day.\n",
    "\n",
    "Let's go back to our bike-sharing dataset.\n",
    "\n",
    "The first column is the index (the global ordering of the data),\n",
    "\n",
    "the second is the date, and the sixth is the time of the day.\n",
    "\n",
    "All we have to do to obtain our daily hours dataset is view the same tensor\n",
    "\n",
    "in batches of 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e16c48ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:14:18.173858Z",
     "start_time": "2021-07-01T08:14:18.166319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17520, 17]), (17, 1))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes.shape, bikes.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3842b7",
   "metadata": {},
   "source": [
    "That's 17,520 hours, 17 columns.\n",
    "\n",
    "Now let's reshape the data to have 3 axes-day, hour, and then our 17 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c3effa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:15:42.732660Z",
     "start_time": "2021-07-01T08:15:42.715662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 24, 17]), (408, 17, 1))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e58a17",
   "metadata": {},
   "source": [
    "What happened here?\n",
    "\n",
    "First, bikes.shape[1] is 17, the number of columns in the bikes tensor.\n",
    "\n",
    "But the real crux of this code is the call to view, which is really important:\n",
    "    \n",
    "it changes the way the tensor looks at the same data as contained in storage.\n",
    "\n",
    "For daily_bikes, the stride is telling us that advancing by 1 along the hour dimension\n",
    "\n",
    "(the second dimension) requires us to advance by 17 places in the storage (or one set of columns);\n",
    "\n",
    "whereas advancing along the day dimension(the first dimension) requires us to\n",
    "\n",
    "advance by a number of elements equal to the length of a row in the storage times 24.\n",
    "\n",
    "We see that the rightmost dimension is the number of columns in the original dataset.\n",
    "\n",
    "Then, in the middle dimension, we have time, split into chunks of 24 sequential hours.\n",
    "\n",
    "In other words, we now have N sequences of L hours in a day, for C channels.\n",
    "\n",
    "To get to our desired N x C x L ordering, we need to transpose the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6893638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:19:45.255609Z",
     "start_time": "2021-07-01T08:19:45.229536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 17, 24]), (408, 1, 17))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = daily_bikes.transpose(1, 2)\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dd25f",
   "metadata": {},
   "source": [
    "### 4.4.3 Ready for training\n",
    "\n",
    "The 'weather situation' variable is ordinal.\n",
    "\n",
    "It has four levels: 1 gor good weather, and 4 for really bad.\n",
    "\n",
    "We could treat this variable as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16945d47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:21:46.975366Z",
     "start_time": "2021-07-01T08:21:46.966806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_day = bikes[:24].long()\n",
    "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
    "first_day[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73fcf3c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:23:41.317208Z",
     "start_time": "2021-07-01T08:23:41.310197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot.scatter_(dim=1,\n",
    "                       index=first_day[:,9].unsqueeze(1).long() - 1,\n",
    "                        # decreases the values by 1 because\n",
    "                        # because weather situation ranges from 1 to 4,\n",
    "                        # while indices are 0-based\n",
    "                       value = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeea9f0",
   "metadata": {},
   "source": [
    "Last, we concatenate our matrix to our original dataset using the cat function.\n",
    "\n",
    "Let's look at the first of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ae6f129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:24:48.303828Z",
     "start_time": "2021-07-01T08:24:48.292823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n",
       "          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n",
       "         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((bikes[:24], weather_onehot), 1)[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bc72d",
   "metadata": {},
   "source": [
    "We could have done the same with the reshaped daily_bikes tensor.\n",
    "\n",
    "Remember that it is shaped (B, C, L), where L=24.\n",
    "\n",
    "We first create the zero tensor, with the same B and L,\n",
    "\n",
    "but with the number of additional columns as C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89f6277c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:26:45.690198Z",
     "start_time": "2021-07-01T08:26:45.675196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4,\n",
    "                                  daily_bikes.shape[2])\n",
    "\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e54dbc76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:27:11.423174Z",
     "start_time": "2021-07-01T08:27:11.404154Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a3456",
   "metadata": {},
   "source": [
    "Other kinds of data look like a time series, in that there is a strict ordering.\n",
    "\n",
    "Top two on the list? Text and audio.\n",
    "\n",
    "We'll take a look at text next, and the \"Conclusion\" section has links to additional examples for audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23ef1c",
   "metadata": {},
   "source": [
    "## 4.5. Representing text\n",
    "\n",
    "Deep learning has taken the field of NLP by storm,\n",
    "\n",
    "particularly using models that repeatedly consume a combination of\n",
    "\n",
    "new input and previous model output.\n",
    "\n",
    "These models are called recurrent neural networks (RNNs),\n",
    "\n",
    "and they have been applied with great success to text categorization,\n",
    "\n",
    "text generation, and automated translation systems.\n",
    "\n",
    "### 4.5.1. Converting text to numbers\n",
    "\n",
    "There are two particularly intuitive levels at which networks operate on text:\n",
    "    \n",
    "at the character level, by processing one character at a time,\n",
    "\n",
    "and at the word level, where individual words are the finest-grained entities to be seen by the network.\n",
    "\n",
    "Let's start with a character-level example.\n",
    "\n",
    "First, let's get some text to process.\n",
    "\n",
    "An amazing resource here is Project Gutenberg.\n",
    "\n",
    "Let's load Jane Austen's Pride and Prejudice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2a3ec1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:34:00.627765Z",
     "start_time": "2021-07-01T08:34:00.613762Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/p1ch4/jane-austen/1342-0.txt', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655f6ff",
   "metadata": {},
   "source": [
    "### 4.5.2 One-hot-encoding characters\n",
    "\n",
    "We are going to one-hot encode our characters.\n",
    "\n",
    "It is instrumental to limit the one-hot encoding to a character set\n",
    "\n",
    "that is useful for the text being analyzed.\n",
    "\n",
    "In our case, since we loaded text in English, it is safe to use ASCII and deal with a small encoding.\n",
    "\n",
    "We could also make all of the characters lowercase, to reduce the number of different characters in our encoding.\n",
    "\n",
    "Similarly, we could screen out puncuation, numbers, or other characters that aren't relevant to our expected kinds of text.\n",
    "\n",
    "We first split our text into a list of lines and pick an arbitrary line to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5afa6cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:37:49.430464Z",
     "start_time": "2021-07-01T08:37:49.412450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split('\\n')\n",
    "line = lines[200]\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d66f2f",
   "metadata": {},
   "source": [
    "Let's create a tensor that can hold the total number of one-hot-encoded characters for the whole line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0913094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:39:10.287434Z",
     "start_time": "2021-07-01T08:39:10.269440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 128])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_t = torch.zeros(len(line), 128) # 128 hardcoded due to the limits of ASCII\n",
    "letter_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a7867922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:41:42.490186Z",
     "start_time": "2021-07-01T08:41:42.471182Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    # The text uses directional double quotes, which are not valid ASCII,\n",
    "    # so we screen them out here.\n",
    "    letter_t[i][letter_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea25dc0",
   "metadata": {},
   "source": [
    "### 4.5.3 One-hot encoding whole words\n",
    "\n",
    "We'll define 'clean_words', which takes text and returns it in lowercase and strippped of punctuation.\n",
    "\n",
    "When we call it on our 'Impossible, Mr.Bennet' line, we get the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7d42ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:49:41.491851Z",
     "start_time": "2021-07-01T08:49:41.475848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him',\n",
       " ['“impossible',\n",
       "  'mr',\n",
       "  'bennet',\n",
       "  'impossible',\n",
       "  'when',\n",
       "  'i',\n",
       "  'am',\n",
       "  'not',\n",
       "  'acquainted',\n",
       "  'with',\n",
       "  'him'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_words(input_str):\n",
    "    punctuation = '.,;:\"!?_-' # 이 부분 문제가 있다...안에 더 포함돼야 하는데\n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list]\n",
    "    return word_list\n",
    "\n",
    "words_in_line = clean_words(line)\n",
    "line, words_in_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ced97",
   "metadata": {},
   "source": [
    "Next, let's build a mapping of words to indexes in our encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3845dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:47:48.610323Z",
     "start_time": "2021-07-01T08:47:48.556310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8484, 3828)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sorted(set(clean_words(text)))\n",
    "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
    "\n",
    "len(word2index_dict), word2index_dict['impossible']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad6176",
   "metadata": {},
   "source": [
    "Note that word2index_dict is now a dictionary with words as keys and integer as a value.\n",
    "\n",
    "We will use it to efficienntly find the index of a word as we one-hot encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7583a529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T08:52:58.499954Z",
     "start_time": "2021-07-01T08:52:58.490457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 8324 “impossible\n",
      " 1 4905 mr\n",
      " 2  891 bennet\n",
      " 3 3828 impossible\n",
      " 4 8017 when\n",
      " 5 3740 i\n",
      " 6  445 am\n",
      " 7 5054 not\n",
      " 8  247 acquainted\n",
      " 9 8094 with\n",
      "10 3619 him\n",
      "torch.Size([11, 8484])\n"
     ]
    }
   ],
   "source": [
    "word_t = torch.zeros(len(words_in_line), len(word2index_dict))\n",
    "\n",
    "for i, word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word]\n",
    "    word_t[i][word_index] = 1\n",
    "    print('{:2} {:4} {}'.format(i, word_index, word))\n",
    "    \n",
    "print(word_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42e26d",
   "metadata": {},
   "source": [
    "The choice between character-level and word-level encoding leaves us to make a trade-off.\n",
    "\n",
    "### 4.5.4 Text embeddings\n",
    "\n",
    "### 4.5.5 Text embeddings as a blueprint\n",
    "\n",
    "## 4.6 Conclusion\n",
    "\n",
    "We learned to load the most common types of data and shape them for consumption by a neural network.\n",
    "\n",
    "Now that we're familiar with tensors and how to store data in them,\n",
    "\n",
    "we can move on to the next step toward the goal of the book: teaching you to train deep neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
